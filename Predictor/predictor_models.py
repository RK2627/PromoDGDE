# predictor_models.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class WeightedMSELoss(nn.Module):
    def __init__(self, weight):
        super(WeightedMSELoss, self).__init__()
        self.weight = weight

    def forward(self, y_pred, y_true):
        loss = torch.mean(self.weight * (y_true - y_pred)**2)
        return loss

class NonLocalBlock1D(nn.Module):
    def __init__(self, in_channels, inter_channels=None):
        super(NonLocalBlock1D, self).__init__()

        self.in_channels = in_channels
        self.inter_channels = inter_channels

        if self.inter_channels is None:
            self.inter_channels = in_channels // 2
            if self.inter_channels == 0:
                self.inter_channels = 1

        self.g = nn.Conv1d(in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0)

        self.W = nn.Sequential(
            # nn.BatchNorm1d(self.inter_channels),
            nn.Conv1d(self.inter_channels, in_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm1d(in_channels),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=1, stride=1),
        )
        nn.init.constant_(self.W[1].weight, 0)
        nn.init.constant_(self.W[1].bias, 0)

        self.theta = nn.Conv1d(in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0)
        self.phi = nn.Conv1d(in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        '''
        :param x: (b, c, t, h, w)
        :return:
        '''
        batch_size = x.size(0)

        g_x = self.g(x).view(batch_size, self.inter_channels, -1)
        g_x = g_x.permute(0, 2, 1)

        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
        theta_x = theta_x.permute(0, 2, 1)
        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
        f = torch.matmul(theta_x, phi_x)
        f_div_C = F.softmax(f, dim=-1)

        y = torch.matmul(f_div_C, g_x)
        y = y.permute(0, 2, 1).contiguous()
        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
        W_y = self.W(y)
        z = W_y + x

        return z

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5, lambda_l2=1e-5):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        # CNN Layers with gradually decreasing kernel sizes
        self.cnn1 = nn.Sequential(
            nn.BatchNorm1d(input_size),
            nn.Conv1d(input_size, hidden_size, kernel_size=17, stride=1, padding=4),
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2)
        )

        self.cnn2 = nn.Sequential(
            nn.BatchNorm1d(hidden_size),
            nn.Conv1d(hidden_size, hidden_size, kernel_size=13, stride=1, padding=3),  # Medium kernel size
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2)
        )

        self.cnn3 = nn.Sequential(
            nn.BatchNorm1d(hidden_size),
            nn.Conv1d(hidden_size, hidden_size, kernel_size=9, stride=1, padding=2),  # Smaller kernel size
            nn.ReLU(),
            nn.MaxPool1d(kernel_size=2, stride=2)
        )

        # self.cnn4 = nn.Sequential(
        #     nn.BatchNorm1d(hidden_size),
        #     nn.Conv1d(hidden_size, hidden_size, kernel_size=7, stride=1, padding=1),
        #     # Smallest kernel for local features
        #     nn.ReLU(),
        #     nn.MaxPool1d(kernel_size=2, stride=2)
        # )

        self.lambda_l2 = lambda_l2
        self.non_local_block = NonLocalBlock1D(hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.weight = nn.Parameter(torch.Tensor([0.8]))
        self.use_cuda = True if torch.cuda.is_available() else False

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.cnn1(x)
        x = self.cnn2(x)
        x = self.cnn3(x)
        # x = self.cnn4(x)
        x = self.non_local_block(x)
        x = x.permute(0, 2, 1)
        output, (hidden, _) = self.lstm(x)
        hidden = hidden[-1]
        output = hidden.view(hidden.size(0), -1)
        output = self.fc1(output)
        output = self.relu(output)
        output = self.dropout(output)
        output = self.fc2(output)
        output = output * self.weight  
        return output


